{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage par Renforcement - Programmation Dynamique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auteurs** :  \n",
    "BIZZOZZERO Nicolas  \n",
    "ADOUM Robert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/nfs/Etu3/3504923/Documents/courses/[5I853] FDMS - Fouille de Données et Médias Sociaux/TD-TME/TME8/randomAgent.py:4: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 497, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1424, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.5/asyncio/events.py\", line 126, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-81263168122a>\", line 7, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"TkAgg\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import envs\n",
    "from randomAgent import RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(function=RandomAgent, env_id='gridworld-v0', outdir_path='gridworld-v0/random-agent-results',\n",
    "                  plan_path=\"gridworldPlans/plan0.txt\", dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1},\n",
    "                  episode_count=10000, gamma=0.1, verbose=False, show_agent=True, seed=None):\n",
    "    # You can set the level to logger.DEBUG or logger.WARN if you\n",
    "    # want to change the amount of output.\n",
    "    logger.set_level(logger.INFO)\n",
    "\n",
    "    envx = gym.make(env_id)\n",
    "    outdir = outdir_path \n",
    "    env = wrappers.Monitor(envx, directory=outdir, force=True, video_callable=False)\n",
    "    \n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    reward = 0\n",
    "    done = False\n",
    "    envx.verbose = True\n",
    "    \n",
    "    moyenne_score = 0\n",
    "    moyenne_actions = 0\n",
    "\n",
    "    envx.setPlan(plan_path, dico_rewards)\n",
    "    if function == RandomAgent:\n",
    "        agent = function(envx.action_space)\n",
    "    else:\n",
    "        agent = function(envx.action_space , envx.getMDP(), gamma=gamma)\n",
    "\n",
    "    rsum=0\n",
    "    for i in range(episode_count):\n",
    "        ob = env.reset()\n",
    "\n",
    "        if i % 100 == 0 and i > 0 and show_agent:\n",
    "            envx.verbose = True\n",
    "        else:\n",
    "            envx.verbose = False\n",
    "\n",
    "        if envx.verbose:\n",
    "            envx.render(1)\n",
    "        j = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.act(ob, reward, done)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            \n",
    "            rsum += reward\n",
    "            j += 1\n",
    "            if envx.verbose:\n",
    "                envx.render()\n",
    "            if done:\n",
    "                if verbose:\n",
    "                    print(str(i), \"rsum=\" + str(rsum) + \",\", str(j), \"actions\")\n",
    "                moyenne_actions += j\n",
    "                moyenne_score += rsum\n",
    "                \n",
    "                rsum=0\n",
    "                break\n",
    "    env.close()\n",
    "    return moyenne_actions / episode_count, moyenne_score / episode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent(object):\n",
    "    def __init__(self, action_space, MDP, eps= 0.000001, gamma=0.9):\n",
    "        self.eps = eps\n",
    "        self.action_space = action_space\n",
    "        self.all_states = [s for s in MDP[0]] # Ici sont compris les etats terminaux\n",
    "        self.state = [s for s in MDP[1]] # Ici non\n",
    "        self.P = MDP[1]\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.pi = {s: None for s in self.state} \n",
    "        pi_kplus = {s: self.action_space.sample() for s in self.state}\n",
    "\n",
    "        while self.pi != pi_kplus:\n",
    "            self.pi = dict(pi_kplus) # Copie\n",
    "            Vpi_t = np.array([np.random.random(1)[0] for _ in range(len(self.all_states))])\n",
    "            Vpi_tplus = np.array([0. for i in range(len(self.all_states))])\n",
    "\n",
    "            for s in self.state:\n",
    "                Vpi_tplus[self.state.index(s)] = np.sum([proba*(reward + self.gamma*Vpi_t[self.all_states.index(s_prime)]) \n",
    "                    for proba, s_prime, reward, boolean in self.P[s][self.pi[s]]])\n",
    "\n",
    "            while np.linalg.norm((Vpi_t - Vpi_tplus), ord=np.inf) > self.eps:\n",
    "                Vpi_t = Vpi_tplus.copy()\n",
    "                for s in self.state:\n",
    "                    Vpi_tplus[self.state.index(s)] = np.sum([proba*(reward + self.gamma*Vpi_t[self.all_states.index(s_prime)]) \n",
    "                        for proba, s_prime, reward, boolean in self.P[s][self.pi[s]]])\n",
    "        \n",
    "            for s in self.state:\n",
    "                pi_kplus[s] = np.argmax(\n",
    "                    [np.sum([proba * (reward + self.gamma * Vpi_t[self.all_states.index(s_prime)]) \n",
    "                        for proba, s_prime, reward, boolean in self.P[s][a]]) for a in range(self.action_space.n)])\n",
    "        \n",
    "        self.pi = dict(pi_kplus)\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        return self.pi[observation.dumps()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(object):\n",
    "    def __init__(self, action_space, MDP, eps= 0.000001, gamma=0.09):\n",
    "        self.eps = eps\n",
    "        self.action_space = action_space\n",
    "        self.all_states = [s for s in MDP[0]] # Ici sont compris les etats terminaux\n",
    "        self.state = [s for s in MDP[1]] # Ici non\n",
    "        self.P = MDP[1]\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.pi = {s: None for s in self.state} \n",
    "        \n",
    "        Vpi_t = np.array([np.random.random(1)[0] for _ in range(len(self.all_states))])\n",
    "        Vpi_tplus = np.array([0. for i in range(len(self.all_states))])\n",
    "\n",
    "        for s in self.state:\n",
    "            Vpi_tplus[self.state.index(s)] = np.max([np.sum([proba*(reward + self.gamma*Vpi_t[self.all_states.index(s_prime)]) \n",
    "                for proba, s_prime, reward, boolean in self.P[s][a]]) for a in range(self.action_space.n)])\n",
    "\n",
    "        while np.linalg.norm((Vpi_t - Vpi_tplus), ord=np.inf) > self.eps:\n",
    "            Vpi_t = Vpi_tplus.copy()\n",
    "            for s in self.state:\n",
    "                Vpi_tplus[self.state.index(s)] = np.max([np.sum([proba*(reward + self.gamma*Vpi_t[self.all_states.index(s_prime)]) \n",
    "                for proba, s_prime, reward, boolean in self.P[s][a]]) for a in range(self.action_space.n)])\n",
    "        \n",
    "        for s in self.state:\n",
    "            self.pi[s] = np.argmax(\n",
    "                [np.sum([proba * (reward + self.gamma * Vpi_t[self.all_states.index(s_prime)]) \n",
    "                    for proba, s_prime, reward, boolean in self.P[s][a]]) for a in range(self.action_space.n)])\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        return self.pi[observation.dumps()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche du meilleur $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_research(function, plan_path, min_gamma=0, max_gamma=1, step=0.1, verbose=True):\n",
    "    list_actions = []\n",
    "    list_score = []\n",
    "    gamma = min_gamma\n",
    "    while gamma <= 1:\n",
    "        if verbose:\n",
    "            print(function, \"gamma\", gamma)\n",
    "        a, c = test_function(function=function, show_agent=False, gamma=gamma)\n",
    "        list_actions.append(a)\n",
    "        list_score.append(c)\n",
    "        gamma += step\n",
    "    return list_actions, list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(min_gamma=0, max_gamma=1, step=0.1, plan_path=\"gridworldPlans/plan0.txt\", verbose=True):\n",
    "    x = np.arange(min_gamma, max_gamma + step, step)\n",
    "    list_action, list_score = gamma_research(PolicyIterationAgent, plan_path=plan_path,\n",
    "                                    min_gamma=min_gamma, max_gamma=max_gamma, step=step, verbose=verbose)\n",
    "    list_action2, list_score2 = gamma_research(ValueIterationAgent, plan_path=plan_path,\n",
    "                                    min_gamma=min_gamma, max_gamma=max_gamma, step=step, verbose=verbose)\n",
    "    plt.plot(x, list_action, 'r-', label='Policy Iteration Nombre d\\'action moyen')\n",
    "    plt.plot(x, list_score,  'r--', label='Policy Iteration Score moyen')\n",
    "    \n",
    "    plt.plot(x, list_action2, 'b-', label='Value Iteration Nombre d\\'action moyen')\n",
    "    plt.plot(x, list_score2,  'b--', label='Value Iteration Score moyen')\n",
    "    plt.xlabel(\"gamma\")\n",
    "    plt.legend()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "graph(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de plusieurs politiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: gridworld-v0\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\Nicolas\\\\Documents\\\\courses\\\\[5I853] FDMS - Fouille de Données et Médias Sociaux\\\\TD-TME\\\\TME8\\\\gridworld-v0\\\\random-agent-results')\n",
      "Politique : Random\n",
      "Moyenne actions : 11.0157 \n",
      "Moyenne scores : -0.7824157000000278 \n",
      "Temps : 9.709 seconde(s).\n",
      "\n",
      "INFO: Making new env: gridworld-v0\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\Nicolas\\\\Documents\\\\courses\\\\[5I853] FDMS - Fouille de Données et Médias Sociaux\\\\TD-TME\\\\TME8\\\\gridworld-v0\\\\random-agent-results')\n",
      "Politique : Policy Iteration\n",
      "Moyenne actions : 28.0363 \n",
      "Moyenne scores : 0.9729636999999933 \n",
      "Temps : 45.42 seconde(s).\n",
      "\n",
      "INFO: Making new env: gridworld-v0\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\Nicolas\\\\Documents\\\\courses\\\\[5I853] FDMS - Fouille de Données et Médias Sociaux\\\\TD-TME\\\\TME8\\\\gridworld-v0\\\\random-agent-results')\n",
      "Politique : Value Iteration\n",
      "Moyenne actions : 28.0706 \n",
      "Moyenne scores : 0.9729293999999944 \n",
      "Temps : 45.632 seconde(s).\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "res = test_function(function=RandomAgent, show_agent=False, seed=0,\n",
    "                    outdir_path='gridworld-v0/random-agent-results',\n",
    "                    plan_path=\"gridworldPlans/plan0.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Politique : Random\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    outdir_path='gridworld-v0/random-agent-results',\n",
    "                    plan_path=\"gridworldPlans/plan0.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Politique : Policy Iteration\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=ValueIterationAgent, show_agent=False, seed=0,\n",
    "                    outdir_path='gridworld-v0/random-agent-results',\n",
    "                    plan_path=\"gridworldPlans/plan0.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Politique : Value Iteration\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux politiques semblent prendre approximativement le même temps pour converger ainsi que les mêmes scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de plusieurs plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: gridworld-v0\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 0 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n', '1 1 1 1 1 1']\n",
      "INFO: Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "['1 1 1 1 1 1\\n', '1 0 0 0 3 1\\n', '1 0 1 4 5 1\\n', '1 0 0 0 2 1\\n', '1 1 1 1 1 1\\n']\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\Nicolas\\\\Documents\\\\courses\\\\[5I853] FDMS - Fouille de Données et Médias Sociaux\\\\TD-TME\\\\TME8\\\\gridworld-v0\\\\random-agent-results')\n",
      "Plan : 1\n",
      "Moyenne actions : 510.1851 \n",
      "Moyenne scores : 0.4950143 \n",
      "Temps : 840.842 seconde(s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan1.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 1\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan2.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 2\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan3.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 3\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan4.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 4\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan5.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 5\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan6.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 6\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan7.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 7\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan8.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 8\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "res = test_function(function=PolicyIterationAgent, show_agent=False, seed=0,\n",
    "                    plan_path=\"gridworldPlans/plan9.txt\",\n",
    "                    dico_rewards={0:-0.001, 3:1, 4:1, 5:-1, 6:-1})\n",
    "t2 = time.time()\n",
    "print(\"Plan : 9\\nMoyenne actions :\", res[0], \"\\nMoyenne scores :\", res[1], \"\\nTemps :\", round(t2 - t1, 3), \"seconde(s).\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
