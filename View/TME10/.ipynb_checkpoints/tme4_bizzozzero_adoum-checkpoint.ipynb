{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDMS, TME4 - Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auteurs :  \n",
    "* BIZZOZZERO Nicolas\n",
    "* ADOUM Robert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from a2c import A2C, init_environment, run_agent_on_environment\n",
    "\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "SEED_ENVIRONMENT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Online A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 last rewards for episode 100 : 11.158 (std 5.325)\n",
      "Mean of 100 last rewards for episode 200 : 11.891 (std 4.298)\n",
      "Mean of 100 last rewards for episode 300 : 18.98 (std 9.625)\n",
      "Mean of 100 last rewards for episode 400 : 10.703 (std 4.197)\n",
      "Mean of 100 last rewards for episode 500 : 11.713 (std 4.368)\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de l'environnement\n",
    "env_id = 'CartPole-v1'\n",
    "env, envx = init_environment(env_id, seed=SEED_ENVIRONMENT)\n",
    "\n",
    "agent = A2C(dim_input=env.observation_space.shape[0],\n",
    "            dim_output=env.action_space.n,\n",
    "            gamma=0.99,\n",
    "            alpha=0.7,\n",
    "            layers=[200],\n",
    "            lr_V=0.001,\n",
    "            lr_pi=0.001)\n",
    "run_agent_on_environment(agent, env, envx,\n",
    "                         max_episode=500,\n",
    "                         iter_print=100,\n",
    "                         iter_show=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 100 last rewards for episode 100 : -257.355 (std 163.734)\n",
      "Mean of 100 last rewards for episode 200 : -373.985 (std 364.512)\n",
      "Mean of 100 last rewards for episode 300 : -509.404 (std 235.436)\n",
      "Mean of 100 last rewards for episode 400 : -306.298 (std 181.438)\n",
      "Mean of 100 last rewards for episode 500 : -274.502 (std 232.843)\n",
      "Mean of 100 last rewards for episode 600 : -419.821 (std 169.033)\n",
      "Mean of 100 last rewards for episode 700 : -411.557 (std 202.405)\n",
      "Mean of 100 last rewards for episode 800 : -478.81 (std 187.233)\n",
      "Mean of 100 last rewards for episode 900 : -479.832 (std 144.999)\n",
      "Mean of 100 last rewards for episode 1000 : -553.223 (std 185.251)\n",
      "Mean of 100 last rewards for episode 1100 : -543.838 (std 174.076)\n",
      "Mean of 100 last rewards for episode 1200 : -578.676 (std 195.022)\n",
      "Mean of 100 last rewards for episode 1300 : -555.656 (std 165.963)\n",
      "Mean of 100 last rewards for episode 1400 : -550.014 (std 153.651)\n",
      "Mean of 100 last rewards for episode 1500 : -555.131 (std 178.046)\n",
      "Mean of 100 last rewards for episode 1600 : -551.221 (std 176.693)\n",
      "Mean of 100 last rewards for episode 1700 : -569.703 (std 157.616)\n",
      "Mean of 100 last rewards for episode 1800 : -600.668 (std 178.108)\n",
      "Mean of 100 last rewards for episode 1900 : -576.469 (std 187.241)\n",
      "Mean of 100 last rewards for episode 2000 : -586.203 (std 176.239)\n",
      "Mean of 100 last rewards for episode 2100 : -558.05 (std 168.669)\n",
      "Mean of 100 last rewards for episode 2200 : -561.352 (std 161.33)\n",
      "Mean of 100 last rewards for episode 2300 : -563.525 (std 175.159)\n",
      "Mean of 100 last rewards for episode 2400 : -584.796 (std 185.732)\n",
      "Mean of 100 last rewards for episode 2500 : -565.866 (std 163.712)\n",
      "Mean of 100 last rewards for episode 2600 : -573.879 (std 185.377)\n",
      "Mean of 100 last rewards for episode 2700 : -568.637 (std 190.166)\n",
      "Mean of 100 last rewards for episode 2800 : -575.949 (std 196.254)\n",
      "Mean of 100 last rewards for episode 2900 : -577.465 (std 172.705)\n",
      "Mean of 100 last rewards for episode 3000 : -556.575 (std 173.515)\n",
      "Mean of 100 last rewards for episode 3100 : -555.308 (std 172.463)\n",
      "Mean of 100 last rewards for episode 3200 : -613.796 (std 193.502)\n",
      "Mean of 100 last rewards for episode 3300 : -538.402 (std 161.084)\n",
      "Mean of 100 last rewards for episode 3400 : -570.842 (std 176.203)\n",
      "Mean of 100 last rewards for episode 3500 : -573.366 (std 180.951)\n",
      "Mean of 100 last rewards for episode 3600 : -549.013 (std 160.387)\n",
      "Mean of 100 last rewards for episode 3700 : -584.328 (std 170.758)\n",
      "Mean of 100 last rewards for episode 3800 : -571.596 (std 168.645)\n",
      "Mean of 100 last rewards for episode 3900 : -545.258 (std 160.058)\n",
      "Mean of 100 last rewards for episode 4000 : -590.171 (std 176.558)\n",
      "Mean of 100 last rewards for episode 4100 : -588.988 (std 192.778)\n",
      "Mean of 100 last rewards for episode 4200 : -566.973 (std 187.265)\n",
      "Mean of 100 last rewards for episode 4300 : -575.19 (std 178.833)\n",
      "Mean of 100 last rewards for episode 4400 : -614.778 (std 197.319)\n",
      "Mean of 100 last rewards for episode 4500 : -557.427 (std 169.766)\n",
      "Mean of 100 last rewards for episode 4600 : -576.252 (std 145.477)\n",
      "Mean of 100 last rewards for episode 4700 : -578.125 (std 192.984)\n",
      "Mean of 100 last rewards for episode 4800 : -574.146 (std 165.915)\n",
      "Mean of 100 last rewards for episode 4900 : -567.782 (std 160.406)\n",
      "Mean of 100 last rewards for episode 5000 : -586.589 (std 178.468)\n",
      "Mean of 100 last rewards for episode 5100 : -564.985 (std 154.461)\n",
      "Mean of 100 last rewards for episode 5200 : -605.674 (std 207.383)\n",
      "Mean of 100 last rewards for episode 5300 : -559.23 (std 174.362)\n",
      "Mean of 100 last rewards for episode 5400 : -543.302 (std 163.81)\n",
      "Mean of 100 last rewards for episode 5500 : -580.608 (std 191.555)\n",
      "Mean of 100 last rewards for episode 5600 : -559.082 (std 168.14)\n",
      "Mean of 100 last rewards for episode 5700 : -592.541 (std 181.943)\n",
      "Mean of 100 last rewards for episode 5800 : -556.004 (std 175.008)\n",
      "Mean of 100 last rewards for episode 5900 : -538.415 (std 137.67)\n",
      "Mean of 100 last rewards for episode 6000 : -555.888 (std 182.304)\n",
      "Mean of 100 last rewards for episode 6100 : -585.057 (std 177.972)\n",
      "Mean of 100 last rewards for episode 6200 : -559.28 (std 153.823)\n",
      "Mean of 100 last rewards for episode 6300 : -551.744 (std 165.264)\n",
      "Mean of 100 last rewards for episode 6400 : -581.684 (std 166.258)\n",
      "Mean of 100 last rewards for episode 6500 : -604.997 (std 190.394)\n",
      "Mean of 100 last rewards for episode 6600 : -574.303 (std 169.567)\n",
      "Mean of 100 last rewards for episode 6700 : -593.15 (std 164.895)\n",
      "Mean of 100 last rewards for episode 6800 : -563.454 (std 190.828)\n",
      "Mean of 100 last rewards for episode 6900 : -574.217 (std 169.716)\n",
      "Mean of 100 last rewards for episode 7000 : -549.272 (std 170.409)\n",
      "Mean of 100 last rewards for episode 7100 : -572.769 (std 160.127)\n",
      "Mean of 100 last rewards for episode 7200 : -558.675 (std 156.72)\n",
      "Mean of 100 last rewards for episode 7300 : -529.264 (std 149.842)\n",
      "Mean of 100 last rewards for episode 7400 : -592.695 (std 175.743)\n",
      "Mean of 100 last rewards for episode 7500 : -579.75 (std 189.362)\n",
      "Mean of 100 last rewards for episode 7600 : -571.622 (std 177.058)\n",
      "Mean of 100 last rewards for episode 7700 : -574.729 (std 165.36)\n",
      "Mean of 100 last rewards for episode 7800 : -604.046 (std 188.275)\n",
      "Mean of 100 last rewards for episode 7900 : -558.528 (std 169.855)\n",
      "Mean of 100 last rewards for episode 8000 : -565.721 (std 156.243)\n",
      "Mean of 100 last rewards for episode 8100 : -555.131 (std 168.195)\n",
      "Mean of 100 last rewards for episode 8200 : -581.638 (std 173.606)\n",
      "Mean of 100 last rewards for episode 8300 : -559.861 (std 178.947)\n",
      "Mean of 100 last rewards for episode 8400 : -558.647 (std 195.203)\n",
      "Mean of 100 last rewards for episode 8500 : -584.298 (std 179.877)\n",
      "Mean of 100 last rewards for episode 8600 : -592.843 (std 190.417)\n",
      "Mean of 100 last rewards for episode 8700 : -555.862 (std 158.28)\n",
      "Mean of 100 last rewards for episode 8800 : -574.723 (std 177.184)\n",
      "Mean of 100 last rewards for episode 8900 : -565.781 (std 170.718)\n",
      "Mean of 100 last rewards for episode 9000 : -582.55 (std 172.722)\n",
      "Mean of 100 last rewards for episode 9100 : -555.735 (std 171.541)\n",
      "Mean of 100 last rewards for episode 9200 : -562.286 (std 171.064)\n",
      "Mean of 100 last rewards for episode 9300 : -571.437 (std 175.047)\n",
      "Mean of 100 last rewards for episode 9400 : -563.543 (std 180.034)\n",
      "Mean of 100 last rewards for episode 9500 : -559.738 (std 160.761)\n",
      "Mean of 100 last rewards for episode 9600 : -592.273 (std 197.122)\n",
      "Mean of 100 last rewards for episode 9700 : -550.303 (std 161.899)\n",
      "Mean of 100 last rewards for episode 9800 : -581.128 (std 213.209)\n",
      "Mean of 100 last rewards for episode 9900 : -605.845 (std 172.55)\n",
      "Mean of 100 last rewards for episode 10000 : -579.055 (std 167.07)\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de l'environnement\n",
    "env_id = 'LunarLander-v2'\n",
    "env, envx = init_environment(env_id, seed=SEED_ENVIRONMENT)\n",
    "\n",
    "agent = A2C(dim_input=env.observation_space.shape[0],\n",
    "            dim_output=env.action_space.n,\n",
    "            gamma=0.99,\n",
    "            alpha=0.7,\n",
    "            layers=[30, 30],\n",
    "            lr_V=0.01,\n",
    "            lr_pi=0.001)\n",
    "run_agent_on_environment(agent, env, envx,\n",
    "                         max_episode=10000,\n",
    "                         iter_print=100,\n",
    "                         iter_show=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Batch A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
